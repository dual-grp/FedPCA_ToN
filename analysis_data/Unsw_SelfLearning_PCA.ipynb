{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pathlib import Path\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_train = os.path.join(os.path.abspath('..'), \"abnormal_detection_data/train/unswnb15_train_normal.csv\")\n",
    "file_path_test_normal = os.path.join(os.path.abspath('..'), \"abnormal_detection_data/test/unswnb15_test_normal_full.csv\")\n",
    "file_path_test_abnormal = os.path.join(os.path.abspath('..'), \"abnormal_detection_data/test/unswnb15_test_abnormal.csv\")\n",
    "df_train = pd.read_csv(file_path_train, index_col = 0)\n",
    "df_test_normal = pd.read_csv(file_path_test_normal, index_col = 0)\n",
    "df_test_abnormal = pd.read_csv(file_path_test_abnormal, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_normal = df_test_normal[:20000]\n",
    "df_test = pd.concat([df_test_normal, df_test_abnormal])\n",
    "df_test.columns = df_test_abnormal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45332, 39), (20000, 39))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_abnormal.shape, df_test_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the score function for abnormal detection\n",
    "def anomalyScores(originalDF, reducedDF):\n",
    "  loss = np.sum((np.array(originalDF)-np.array(reducedDF))**2, axis=1)\n",
    "  loss = pd.Series(data=loss,index=originalDF.index)\n",
    "  loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def results_analysis(df_gt_score, threshold, log=0):\n",
    "  df_gt_pred = pd.DataFrame()\n",
    "  df_gt_pred['ground_true'] = df_gt_score['ground_true']\n",
    "  index = df_gt_score['anomalyScore'] > threshold\n",
    "  df_gt_pred['prediction'] = index.astype(int)\n",
    "\n",
    "  TN, FP, FN, TP = confusion_matrix(df_gt_pred['ground_true'], df_gt_pred['prediction']).ravel()\n",
    "  precision_score = TP/(FP + TP)\n",
    "  recall_score = TP/(FN + TP)\n",
    "  accuracy_score = (TP + TN)/ (TP + FN + TN + FP)\n",
    "  f1_score = 2*precision_score*recall_score/(precision_score + recall_score)\n",
    "  fpr = FP / (FP+TN)\n",
    "  fng = FN / (TP+FN)\n",
    "\n",
    "  if log:\n",
    "    print(f\"Precision: {np.round(precision_score * 100.0,4)}%\")\n",
    "    print(f\"Recall: {np.round(recall_score * 100.0,4)}%\")\n",
    "    print(f\"Accuracy score: {np.round(accuracy_score * 100.0,4)}%\")\n",
    "    print(f\"F1 score: {np.round(f1_score * 100.0,4)}%\")\n",
    "    print(f\"False alarm: {np.round(fpr * 100.0,4)}%\")\n",
    "    print(f\"False Negative: {np.round(fng * 100.0,4)}%\")\n",
    "\n",
    "  return precision_score, recall_score, accuracy_score, f1_score, fpr, fng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "scaler = StandardScaler()\n",
    "def perform_pca(df_train, df_test):\n",
    "  pca = PCA(0.99)\n",
    "  pca.fit(scaler.transform(df_train))\n",
    "  df_test_PCA = pca.transform(df_test)\n",
    "  df_test_PCA_inverse = pca.inverse_transform(df_test_PCA)\n",
    "  df_test_PCA = pd.DataFrame(df_test_PCA)\n",
    "  df_test_PCA_inverse = pd.DataFrame(df_test_PCA_inverse)\n",
    "  return df_test_PCA, df_test_PCA_inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change number of user in the cell below for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_raw = df_test.copy()\n",
    "# df_normal_train = df_train.copy()\n",
    "# num_users = 40\n",
    "# fraction  = int(df_normal_train.shape[0] / num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_normal_train = df_normal_train.sort_values(by=['ct_srv_src'])\n",
    "# df_normal_train = df_normal_train.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_raw = df_test.copy()\n",
    "df_normal_train = df_train.copy()\n",
    "df_normal_train = df_normal_train.sort_values(by=['ct_srv_src'])\n",
    "df_normal_train = df_normal_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Results for 10--------------------\n",
      "Precision: 74.5887%\n",
      "Recall: 53.7437%\n",
      "Accuracy score: 55.199%\n",
      "F1 score: 62.4733%\n",
      "False alarm: 41.5025%\n",
      "False Negative: 46.2563%\n",
      "--------------------Results for 20--------------------\n",
      "Precision: 71.8078%\n",
      "Recall: 51.7289%\n",
      "Accuracy score: 52.4137%\n",
      "F1 score: 60.1366%\n",
      "False alarm: 46.0342%\n",
      "False Negative: 48.2711%\n",
      "--------------------Results for 30--------------------\n",
      "Precision: 73.7471%\n",
      "Recall: 53.1358%\n",
      "Accuracy score: 54.3571%\n",
      "F1 score: 61.7673%\n",
      "False alarm: 42.8747%\n",
      "False Negative: 46.8642%\n",
      "--------------------Results for 40--------------------\n",
      "Precision: 72.1391%\n",
      "Recall: 51.9732%\n",
      "Accuracy score: 52.7472%\n",
      "F1 score: 60.4178%\n",
      "False alarm: 45.4982%\n",
      "False Negative: 48.0268%\n",
      "--------------------Results for 50--------------------\n",
      "Precision: 71.4322%\n",
      "Recall: 51.4644%\n",
      "Accuracy score: 52.0405%\n",
      "F1 score: 59.8262%\n",
      "False alarm: 46.6537%\n",
      "False Negative: 48.5356%\n",
      "--------------------Results for 60--------------------\n",
      "Precision: 70.9751%\n",
      "Recall: 51.1358%\n",
      "Accuracy score: 51.5842%\n",
      "F1 score: 59.4438%\n",
      "False alarm: 47.3994%\n",
      "False Negative: 48.8642%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_experiments = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "for num_users in user_experiments:\n",
    "  fraction  = int(df_normal_train.shape[0] / num_users)\n",
    "  avg_acc = 0\n",
    "  avg_pre = 0\n",
    "  avg_rec = 0\n",
    "  avg_f1 = 0\n",
    "  avg_fpr = 0\n",
    "  avg_fng = 0\n",
    "  for i in range(num_users):\n",
    "    df_train_stdPCA = df_normal_train[fraction*i:fraction*(i+1)].copy()\n",
    "    df_train_client = df_train_stdPCA.copy()\n",
    "    # Standardization over Testing\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_train_client.to_numpy())\n",
    "    df_test = pd.DataFrame(scaler.transform(df_test_raw.to_numpy()))\n",
    "    df_test.columns = df_test_abnormal.columns\n",
    "    _, df_test_PCA_inverse = perform_pca(df_train_client.to_numpy(), df_test.to_numpy())\n",
    "\n",
    "    abnormal_score = anomalyScores(df_test, df_test_PCA_inverse)\n",
    "\n",
    "    df_gt_score_PCA = pd.DataFrame(); df_gt_pred_PCA = pd.DataFrame()\n",
    "    df_gt_score_PCA['ground_true'] = np.concatenate([np.zeros(len(df_test_normal)), np.ones(len(df_test_abnormal))])\n",
    "    df_gt_score_PCA['anomalyScore'] = abnormal_score\n",
    "\n",
    "    # choose the right threshold\n",
    "    lst_p = np.arange(1e-1,9e-1,1e-1) # Among test, ratio of normal/abnormal = 0.75\n",
    "    lst_rho = np.quantile(df_gt_score_PCA.anomalyScore, lst_p)\n",
    "    optimal_p = 5e-1\n",
    "    optimal_rho = lst_rho[abs(lst_p - optimal_p)<1e-8][0]\n",
    "    \n",
    "    precision_score, recall_score, accuracy_score, f1_score, fpr, fng = results_analysis(df_gt_score_PCA, threshold=optimal_rho, log=0)\n",
    "    avg_acc += accuracy_score\n",
    "    avg_pre += precision_score\n",
    "    avg_rec += recall_score\n",
    "    avg_f1 += f1_score\n",
    "    avg_fpr += fpr\n",
    "    avg_fng += fng\n",
    "  print(f\"--------------------Average results for {num_users} users--------------------\")\n",
    "  print(f\"Precision: {np.round(avg_pre * 100.0/num_users,4)}%\")\n",
    "  print(f\"Recall: {np.round(avg_rec * 100.0/num_users,4)}%\")\n",
    "  print(f\"Accuracy score: {np.round(avg_acc * 100.0/num_users,4)}%\")\n",
    "  print(f\"F1 score: {np.round(avg_f1 * 100.0/num_users,4)}%\")\n",
    "  print(f\"False alarm: {np.round(avg_fpr * 100.0/num_users,4)}%\")\n",
    "  print(f\"False Negative: {np.round(avg_fng * 100.0/num_users,4)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
